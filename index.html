<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Hang Guo (éƒ­èˆª) </title> <meta name="author" content="Hang Guo (éƒ­èˆª)"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%95&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://csguoh.github.io/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%63%73%68%67%75%6F@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=fRwhfpoAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/csguoh" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/hang-guo-59690a2a2" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/csguoh" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://www.zhihu.com/people/hang-hang-oc" title="Blogger" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-blogger-b"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/award/">Awards </a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">Service </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Hang</span> Guo (éƒ­èˆª) </h1> <p class="desc">Master student @Tsinghua University</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_img-480.webp 480w,/assets/img/prof_img-800.webp 800w,/assets/img/prof_img-1400.webp 1400w," sizes="(min-width: 1000px) 291.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_img.jpg?f4aa1181c7072a7820b18a8aa9ddb7e2" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_img.jpg" loading="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p></p> <p></p> <p></p> </div> </div> <div class="clearfix"> <p>Hi, there ðŸ‘‹</p> <p>I am a Master student at <a href="https://www.sigs.tsinghua.edu.cn/en/" rel="external nofollow noopener" target="_blank">Tsinghua Shenzhen International Graduate School (SIGS)</a>, <a href="https://www.tsinghua.edu.cn/en/index.htm" rel="external nofollow noopener" target="_blank">Tsinghua University</a>, advised by Prof. <a href="https://www.sigs.tsinghua.edu.cn/xst_en/main.htm" rel="external nofollow noopener" target="_blank">Shu-tao Xia</a>. I also work closely with Prof. <a href="https://scholar.google.com.hk/citations?user=MqJNdaAAAAAJ&amp;hl=zh-CN&amp;oi=ao" rel="external nofollow noopener" target="_blank">Tao Dai</a> and Prof. <a href="https://yaweili.bitbucket.io/" rel="external nofollow noopener" target="_blank">Yawei Li</a>. Before that, I obtained my dual degree of B.Eng. &amp; B.Ec. from <a href="https://en.nankai.edu.cn/" rel="external nofollow noopener" target="_blank">Nankai University</a>.</p> <p>My research interests focus on:</p> <ul> <li> <strong>Generative Computer Vision</strong>: trying to build a realistic digital world, such as image generation, image restoration, image super-resolution, etc.</li> <li> <strong>Efficient Artificial Intelligence</strong>: trying to allow AI available for everyone, such as parameter efficient fine-tuning, network quantization, sparsity, distillation, etc.</li> </ul> <div class="alert alert-info" style="max-width: 660px; color: #5445b4; background-color: #F0F8FF;"> <span style="color: #5445b4;"> <b>Looking for a PhD Position!</b> <br> I am actively seeking a PhD position or an academic internship. If you are interested, feel free to contact me.ðŸ¤—. </span> </div> <h3 id="contact">Contact</h3> <p>email: cshguo[at]gmail[dot]com</p> <p><em>(Last Update: 2025-06-27)</em></p> <h3 id="news">News</h3> <div class="news"> <div class="table-responsive" style="max-height: 16vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 150px;">June 26, 2025</th> <td> Our <a href="https://github.com/csguoh/FastVAR" target="_blank" rel="external nofollow noopener"><b>FastVAR</b></a> has been accepted by ICCV25ðŸ¥³. </td> </tr> <tr> <th scope="row" style="width: 150px;">June 03, 2025</th> <td> I start my summer internship at <a href="https://www.epfl.ch/en/" target="_blank" rel="external nofollow noopener"><b>EPFL</b></a> in Lausanne:D </td> </tr> <tr> <th scope="row" style="width: 150px;">May 02, 2025</th> <td> Our IntLoRA has been accepted by <strong>ICML2025</strong>! Check our paper <a href="https://arxiv.org/abs/2410.21759" target="_blank" rel="external nofollow noopener"><b>here</b></a> :D </td> </tr> <tr> <th scope="row" style="width: 150px;">Apr 29, 2025</th> <td> Two papers have been accepted by <strong>IJCAI2025</strong>. </td> </tr> <tr> <th scope="row" style="width: 150px;">Mar 28, 2025</th> <td> We release <a href="https://github.com/csguoh/FastVAR" target="_blank" rel="external nofollow noopener"><b>FastVAR</b></a>, a new cached token pruning method for <strong>2.7x faster</strong> Visual Auto-regressive Modeling. </td> </tr> <tr> <th scope="row" style="width: 150px;">Feb 27, 2025</th> <td> Congrats! Our MambaIRv2 has been accepted by <strong>CVPR2025</strong>! </td> </tr> <tr> <th scope="row" style="width: 150px;">Feb 08, 2025</th> <td> We are organizing the CVPR25 Workshop <a href="https://www.cvlai.net/ntire/2025/" target="_blank" rel="external nofollow noopener"><b>NTIRE 2025 Challenge</b></a> on <a href="https://codalab.lisn.upsaclay.fr/competitions/21560" target="_blank" rel="external nofollow noopener"><b> Image Denosing</b></a>. </td> </tr> <tr> <th scope="row" style="width: 150px;">Feb 07, 2025</th> <td> We are organizing the CVPR25 Workshop <a href="https://www.cvlai.net/ntire/2025/" target="_blank" rel="external nofollow noopener"><b>NTIRE 2025 Challenge</b></a> on <a href="https://codalab.lisn.upsaclay.fr/competitions/21620" target="_blank" rel="external nofollow noopener"><b>Efficient Super-Resolution</b></a>. </td> </tr> <tr> <th scope="row" style="width: 150px;">Dec 22, 2024</th> <td> Our work CALF, a LLM-based time series foundation models, has been accepted by <strong>AAAI2025</strong>! </td> </tr> <tr> <th scope="row" style="width: 150px;">Nov 30, 2024</th> <td> Big News! <strong>MambaIRv2</strong> has been Arxived with huge performance leap! Check our paper <a href="http://arxiv.org/abs/2411.15269" target="_blank" rel="external nofollow noopener"><b>here</b></a>. </td> </tr> <tr> <th scope="row" style="width: 150px;">Nov 15, 2024</th> <td> Congratulations! I have been honored as the <strong>Top Reviewer</strong> by <a href="https://neurips.cc/Conferences/2024/ProgramCommittee#top-reviewers" target="_blank" rel="external nofollow noopener">NeurIPS24 PCs</a>! </td> </tr> <tr> <th scope="row" style="width: 150px;">Oct 15, 2024</th> <td> We release <strong>IntLoRA</strong>, which allows LoRA tuning on quantized models. Check our paper <a href="https://arxiv.org/pdf/2410.21759" target="_blank" rel="external nofollow noopener"><b>here</b></a> :D </td> </tr> <tr> <th scope="row" style="width: 150px;">Sep 30, 2024</th> <td> Two first-author works, AdaptIR and ReFIR, have been accepted by <strong>NeurIPS2024</strong>! </td> </tr> <tr> <th scope="row" style="width: 150px;">Jul 03, 2024</th> <td> Important! We have released the <a href="https://github.com/csguoh/Awesome-Mamba-in-Low-Level-Vision" target="_blank" rel="external nofollow noopener"><b>Awesome-Mamba-in-LLV</b></a>, collecting recent Mamba-based methods! </td> </tr> <tr> <th scope="row" style="width: 150px;">Jul 01, 2024</th> <td> The first Mamba-based image restoration backbone, <strong>MambaIR</strong>, has been accepted by <strong>ECCV2024</strong>! </td> </tr> <tr> <th scope="row" style="width: 150px;">Aug 04, 2023</th> <td> One knowledge distillation based OCR for real-world degraded scene was accepted by <strong>MM2023</strong>! </td> </tr> <tr> <th scope="row" style="width: 150px;">Apr 15, 2023</th> <td> One text image super-resolution work has been accepted by <strong>IJCAI2023</strong>! </td> </tr> </table> </div> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICCV</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fastvar-480.webp 480w,/assets/img/publication_preview/fastvar-800.webp 800w,/assets/img/publication_preview/fastvar-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/fastvar.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fastvar.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="guo2025fastvar" class="col-sm-8"> <div class="title">FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning</div> <div class="author"> <em>Hang Guo</em> ,Â  Yawei Li ,Â  Taolin Zhang , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Jiangshan Wang, Tao Dai, Shu-Tao Xia, Luca Benini' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>The IEEE International Conference on Computer Vision (ICCV)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2503.23367" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/csguoh/FastVAR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/csguoh/FastVAR" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/csguoh/FastVAR"> </a> </div> <div class="abstract hidden"> <p>Visual Autoregressive (VAR) modeling has gained popularity for its shift towards next-scale prediction. However, existing VAR paradigms process the entire token map at each scale step, leading to the complexity and runtime scaling dramatically with image resolution. To address this challenge, we propose FastVAR, a post-training acceleration method for efficient resolution scaling with VARs. Our key finding is that the majority of latency arises from the large-scale step where most tokens have already converged. Leveraging this observation, we develop the cached token pruning strategy that only forwards pivotal tokens for scale-specific modeling while using cached tokens from previous scale steps to restore the pruned slots. This significantly reduces the number of forwarded tokens and improves the efficiency at larger resolutions. Experiments show the proposed FastVAR can further speedup FlashAttention-accelerated VAR by 2.7 with negligible performance drop of &lt;1%. We further extend FastVAR to zero-shot generation of higher resolution images. In particular, FastVAR can generate one 2K image with 15GB memory footprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at https://github.com/csguoh/FastVAR.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">guo2025fastvar</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guo, Hang and Li, Yawei and Zhang, Taolin and Wang, Jiangshan and Dai, Tao and Xia, Shu-Tao and Benini, Luca}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The IEEE International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">repostar</span> <span class="p">=</span> <span class="s">{csguoh/FastVAR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mambairv2-480.webp 480w,/assets/img/publication_preview/mambairv2-800.webp 800w,/assets/img/publication_preview/mambairv2-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/mambairv2.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mambairv2.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="guo2024mambairv2" class="col-sm-8"> <div class="title">MambaIRv2: Attentive State Space Restoration</div> <div class="author"> <em>Hang Guo*</em> ,Â  Yong Guo* ,Â  Yaohua Zha , and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Yulun Zhang, Wenbo Li, Tao Dai, Shu-Tao Xia, Yawei Li' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/abs/2411.15269" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://zhuanlan.zhihu.com/p/9375585949" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/csguoh/MambaIR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/csguoh/MambaIR" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/csguoh/MambaIR"> </a> </div> <div class="abstract hidden"> <p>The Mamba-based image restoration backbones have recently demonstrated significant potential in balancing global reception and computational efficiency. However, the inherent causal modeling limitation of Mamba, where each token depends solely on its predecessors in the scanned sequence, restricts the full utilization of pixels across the image and thus presents new challenges in image restoration. In this work, we propose MambaIRv2, which equips Mamba with the non-causal modeling ability similar to ViTs to reach the attentive state space restoration model. Specifically, the proposed attentive state-space equation allows to attend beyond the scanned sequence and facilitate image unfolding with just one single scan. Moreover, we further introduce a semantic-guided neighboring mechanism to encourage interaction between distant but similar pixels. Extensive experiments show our MambaIRv2 outperforms SRFormer by even 0.35dB PSNR for lightweight SR even with 9.3% less parameters and suppresses HAT on classic SR by up to 0.29dB.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">guo2024mambairv2</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MambaIRv2: Attentive State Space Restoration}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guo*, Hang and Guo*, Yong and Zha, Yaohua and Zhang, Yulun and Li, Wenbo and Dai, Tao and Xia, Shu-Tao and Li, Yawei}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">repostar</span> <span class="p">=</span> <span class="s">{csguoh/MambaIR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/intlora-480.webp 480w,/assets/img/publication_preview/intlora-800.webp 800w,/assets/img/publication_preview/intlora-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/intlora.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="intlora.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="guo2024intlora" class="col-sm-8"> <div class="title">IntLoRA: Integral Low-rank Adaptation of Quantized Diffusion Models</div> <div class="author"> <em>Hang Guo</em> ,Â  Yawei Li ,Â  Tao Dai , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Shu-Tao Xia, Luca Benini' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>The International Conference on Machine Learning (ICML)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2410.21759" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/csguoh/IntLoRA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/csguoh/IntLoRA" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/csguoh/IntLoRA"> </a> </div> <div class="abstract hidden"> <p>Fine-tuning large-scale text-to-image diffusion models for various downstream tasks has yielded impressive results. However, the heavy computational burdens of tuning large models prevent personal customization. Recent advances have attempted to employ parameter-efficient fine-tuning (PEFT) techniques to adapt the floating-point (FP) or quantized pre-trained weights. Nonetheless, the adaptation parameters in existing works are still restricted to FP arithmetic, hindering hardware-friendly acceleration. In this work, we propose IntLoRA, to further push the efficiency limits by using integer type (INT) low-rank parameters to adapt the quantized diffusion models. By working in the integer arithmetic, our IntLoRA offers three key advantages: (i) for fine-tuning, the pre-trained weights are quantized, reducing memory usage; (ii) for storage, both pre-trained and low-rank weights are in INT which consumes less disk space; (iii) for inference, IntLoRA weights can be naturally merged into quantized pre-trained weights through efficient integer multiplication or bit-shifting, eliminating additional post-training quantization. Extensive experiments demonstrate that IntLoRA can achieve performance on par with or even superior to the vanilla LoRA, accompanied by significant efficiency improvements.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">guo2024intlora</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{IntLoRA: Integral Low-rank Adaptation of Quantized Diffusion Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guo, Hang and Li, Yawei and Dai, Tao and Xia, Shu-Tao and Benini, Luca}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The International Conference on Machine Learning (ICML)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">repostar</span> <span class="p">=</span> <span class="s">{csguoh/IntLoRA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/adaptir-480.webp 480w,/assets/img/publication_preview/adaptir-800.webp 800w,/assets/img/publication_preview/adaptir-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/adaptir.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="adaptir.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="guo2024adaptir" class="col-sm-8"> <div class="title">Parameter Efficient Adaptation for Image Restoration with Heterogeneous Mixture-of-Experts</div> <div class="author"> <em>Hang Guo</em> ,Â  Tao Dai ,Â  Yuanchao Bai , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Bin Chen, Xudong Ren, Zexuan Zhu, Shu-Tao Xia' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>The Annual Conference on Neural Information Processing Systems (NeurIPS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2312.08881.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/csguoh/AdaptIR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/csguoh/AdaptIR" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/csguoh/AdaptIR"> </a> </div> <div class="abstract hidden"> <p>Pre-training has shown promising results on various image restoration tasks, which is usually followed by full fine-tuning for each specific downstream task (e.g., image denoising). However, such full fine-tuning usually suffers from the problems of heavy computational cost in practice, due to the massive parameters of pre-trained restoration models, thus limiting its real-world applications. Recently, Parameter Efficient Transfer Learning (PETL) offers an efficient alternative solution to full fine-tuning, yet still faces great challenges for pre-trained image restoration models, due to the diversity of different degradations. To address these issues, we propose AdaptIR, a novel parameter efficient transfer learning method for adapting pre-trained restoration models. Specifically, the proposed method consists of a multi-branch inception structure to orthogonally capture local spatial, global spatial, and channel interactions. In this way, it allows powerful representations under a very low parameter budget. Extensive experiments demonstrate that the proposed method can achieve comparable or even better performance than full fine-tuning, while only using 0.6% parameters. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">guo2024adaptir</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Parameter Efficient Adaptation for Image Restoration with Heterogeneous Mixture-of-Experts}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guo, Hang and Dai, Tao and Bai, Yuanchao and Chen, Bin and Ren, Xudong and Zhu, Zexuan and Xia, Shu-Tao}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Annual Conference on Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">repostar</span> <span class="p">=</span> <span class="s">{csguoh/AdaptIR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/refir-480.webp 480w,/assets/img/publication_preview/refir-800.webp 800w,/assets/img/publication_preview/refir-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/refir.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="refir.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="guo2024refir" class="col-sm-8"> <div class="title">ReFIR: Grounding Large Restoration Models with Retrieval Augmentation</div> <div class="author"> <em>Hang Guo</em> ,Â  Tao Dai ,Â  Zhihao Ouyang , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Taolin Zhang, Yaohua Zha, Bin Chen, Shu-Tao Xia' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>The Annual Conference on Neural Information Processing Systems (NeurIPS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2410.05601" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://zhuanlan.zhihu.com/p/1659159986" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/csguoh/ReFIR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/csguoh/ReFIR" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/csguoh/ReFIR"> </a> </div> <div class="abstract hidden"> <p>Recent advances in diffusion-based Large Restoration Models (LRMs) have significantly improved photo-realistic image restoration by leveraging the internal knowledge embedded within model weights. However, existing LRMs often suffer from the hallucination dilemma, i.e., producing incorrect contents or textures when dealing with severe degradations, due to their heavy reliance on limited internal knowledge. In this paper, we propose an orthogonal solution called the Retrieval-augmented Framework for Image Restoration (ReFIR), which incorporates retrieved images as external knowledge to extend the knowledge boundary of existing LRMs in generating details faithful to the original scene. Specifically, we first introduce the nearest neighbor lookup to retrieve content-relevant high-quality images as reference, after which we propose the cross-image injection to modify existing LRMs to utilize high-quality textures from retrieved images. Thanks to the additional external knowledge, our ReFIR can well handle the hallucination challenge and facilitate faithfully results. Extensive experiments demonstrate that ReFIR can achieve not only high-fidelity but also realistic restoration results. Importantly, our ReFIR requires no training and is adaptable to various LRMs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">guo2024refir</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ReFIR: Grounding Large Restoration Models with Retrieval Augmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guo, Hang and Dai, Tao and Ouyang, Zhihao and Zhang, Taolin and Zha, Yaohua and Chen, Bin and Xia, Shu-Tao}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Annual Conference on Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">repostar</span> <span class="p">=</span> <span class="s">{csguoh/ReFIR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ECCV</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mambair-480.webp 480w,/assets/img/publication_preview/mambair-800.webp 800w,/assets/img/publication_preview/mambair-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/mambair.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mambair.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="guo2024mambair" class="col-sm-8"> <div class="title">MambaIR: A Simple Baseline for Image Restoration with State-Space Model</div> <div class="author"> <em>Hang Guo*</em> ,Â  Jinmin Li* ,Â  Tao Dai , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Zhihao Ouyang, Xudong Ren, Shu-Tao Xia' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Proceedings of the European Conference on Computer Vision (ECCV)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/abs/2402.15648" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://zhuanlan.zhihu.com/p/684248751" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/csguoh/MambaIR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/csguoh/MambaIR" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/csguoh/MambaIR"> </a> </div> <div class="abstract hidden"> <p>Recent years have witnessed great progress in image restoration thanks to the advancements in modern deep neural networks e.g. Convolutional Neural Network and Transformer. However, existing restoration backbones are usually limited due to the inherent local reductive bias or quadratic computational complexity. Recently, Selective Structured State Space Model e.g., Mamba, have shown great potential for long-range dependencies modeling with linear complexity, but it is still under-explored in low-level computer vision. In this work, we introduce a simple but strong benchmark model, named MambaIR, for image restoration. In detail, we propose the Residual State Space Block as the core component, which employs convolution and channel attention to enhance capabilities of the vanilla Mamba. In this way, our MambaIR takes advantages of local patch recurrence prior as well as channel interaction to produce restoration-specific feature representation. Extensive experiments demonstrate the superiority of our method, for example, MambaIR outperforms Transformer-based baseline SwinIR by up to 0.36dB, using similar computational cost but with global receptive field.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">guo2024mambair</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MambaIR: A Simple Baseline for Image Restoration with State-Space Model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guo*, Hang and Li*, Jinmin and Dai, Tao and Ouyang, Zhihao and Ren, Xudong and Xia, Shu-Tao}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">repostar</span> <span class="p">=</span> <span class="s">{csguoh/MambaIR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/latta-480.webp 480w,/assets/img/publication_preview/latta-800.webp 800w,/assets/img/publication_preview/latta-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/latta.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="latta.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="liu2024taming" class="col-sm-8"> <div class="title">Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation</div> <div class="author"> Peiyuan Liu* ,Â  <em>Hang Guo*</em> ,Â  Tao Dai , and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Naiqi Li, Jigang Bao, Xudong Ren, Yong Jiang, Shu-Tao Xia' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Association for the Advancement of Artificial Intelligence (AAAI)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/html/2403.07300v1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Hank0626/LLaTA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/Hank0626/LLaTA" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Hank0626/LLaTA"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2024taming</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu*, Peiyuan and Guo*, Hang and Dai, Tao and Li, Naiqi and Bao, Jigang and Ren, Xudong and Jiang, Yong and Xia, Shu-Tao}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Association for the Advancement of Artificial Intelligence (AAAI)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">repostar</span> <span class="p">=</span> <span class="s">{Hank0626/LLaTA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2026 Hang Guo (éƒ­èˆª). Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>