<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Hang Guo (éƒ­èˆª) </title> <meta name="author" content="Hang Guo (éƒ­èˆª)"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%95&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://csguoh.github.io/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%63%73%68%67%75%6F@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=fRwhfpoAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/csguoh" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/hang-guo-59690a2a2" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/csguoh" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://www.zhihu.com/people/hang-hang-oc" title="Blogger" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-blogger-b"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/award/">Awards </a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">Service </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Hang</span> Guo (éƒ­èˆª) </h1> <p class="desc">Master student @Tsinghua University</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_img-480.webp 480w,/assets/img/prof_img-800.webp 800w,/assets/img/prof_img-1400.webp 1400w," sizes="(min-width: 1000px) 291.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_img.jpg?2f1c0788c24ec33f0d863f78f04474e2" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_img.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p></p> <p></p> <p></p> </div> </div> <div class="clearfix"> <p>Hi, there ðŸ‘‹</p> <p>I am a Master student at <a href="https://www.sigs.tsinghua.edu.cn/en/" rel="external nofollow noopener" target="_blank">Tsinghua Shenzhen International Graduate School (SIGS)</a>, <a href="https://www.tsinghua.edu.cn/en/index.htm" rel="external nofollow noopener" target="_blank">Tsinghua University</a>, advised by Prof. <a href="https://www.sigs.tsinghua.edu.cn/xst_en/main.htm" rel="external nofollow noopener" target="_blank">Shu-tao Xia</a>. I also work closely with Prof. <a href="https://scholar.google.com.hk/citations?user=MqJNdaAAAAAJ&amp;hl=zh-CN&amp;oi=ao" rel="external nofollow noopener" target="_blank">Tao Dai</a> and Prof. <a href="https://yaweili.bitbucket.io/" rel="external nofollow noopener" target="_blank">Yawei Li</a>. Before that, I obtained my dual degree of B.Eng. &amp; B.Ec. from <a href="https://en.nankai.edu.cn/" rel="external nofollow noopener" target="_blank">Nankai University</a>.</p> <p>My research interests focus on:</p> <ul> <li> <strong>Generative Computer Vision</strong>: trying to build a realistic digital world, such as image generation, image restoration, image super-resolution, etc.</li> <li> <strong>Efficient Artificial Intelligence</strong>: trying to allow AI available for everyone, such as parameter efficient fine-tuning, network quantization, sparsity, distillation, etc.</li> </ul> <div class="alert alert-info" style="max-width: 660px; color: #5445b4; background-color: #F0F8FF;"> <span style="color: #5445b4;"> <b>Looking for a PhD Position!</b> <br> I am actively seeking a PhD position or an academic internship. If you are interested, feel free to contact me.ðŸ¤—. </span> </div> <h3 id="contact">Contact</h3> <p>email: cshguo[at]gmail[dot]com</p> <h3 id="news">News</h3> <div class="news"> <div class="table-responsive" style="max-height: 16vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 150px;">Feb 27, 2025</th> <td> Congrats! Our MambaIRv2 has been accepted by <strong>CVPR2025</strong>! </td> </tr> <tr> <th scope="row" style="width: 150px;">Feb 08, 2025</th> <td> We are organizing the <a href="https://www.cvlai.net/ntire/2025/" target="_blank" rel="external nofollow noopener"><b>NTIRE 2025 Challenge</b></a> on <a href="https://codalab.lisn.upsaclay.fr/competitions/21560" target="_blank" rel="external nofollow noopener"><b> Image Denosing</b></a>. </td> </tr> <tr> <th scope="row" style="width: 150px;">Feb 07, 2025</th> <td> We are organizing the <a href="https://www.cvlai.net/ntire/2025/" target="_blank" rel="external nofollow noopener"><b>NTIRE 2025 Challenge</b></a> on <a href="https://codalab.lisn.upsaclay.fr/competitions/21620" target="_blank" rel="external nofollow noopener"><b>Efficient Super-Resolution</b></a>. </td> </tr> <tr> <th scope="row" style="width: 150px;">Dec 22, 2024</th> <td> Our work CALF, a LLM-based time series foundation models, has been accepted by <strong>AAAI2025</strong>! </td> </tr> <tr> <th scope="row" style="width: 150px;">Nov 30, 2024</th> <td> Big News! <strong>MambaIRv2</strong> has been Arxived with huge performance leap! Check our paper <a href="http://arxiv.org/abs/2411.15269" target="_blank" rel="external nofollow noopener"><b>here</b></a>. </td> </tr> <tr> <th scope="row" style="width: 150px;">Nov 15, 2024</th> <td> Congratulations! I have been honored as the <strong>Top Reviewer</strong> by <a href="https://neurips.cc/Conferences/2024/ProgramCommittee#top-reviewers" target="_blank" rel="external nofollow noopener">NeurIPS24 PCs</a>! </td> </tr> <tr> <th scope="row" style="width: 150px;">Oct 15, 2024</th> <td> We release <strong>IntLoRA</strong>, which allows LoRA tuning on quantized models. Check our paper <a href="https://arxiv.org/pdf/2410.21759" target="_blank" rel="external nofollow noopener"><b>here</b></a> :D </td> </tr> <tr> <th scope="row" style="width: 150px;">Sep 30, 2024</th> <td> Two first-author works, AdaptIR and ReFIR, have been accepted by <strong>NeurIPS2024</strong>! </td> </tr> <tr> <th scope="row" style="width: 150px;">Jul 03, 2024</th> <td> Important! We have released the <a href="https://github.com/csguoh/Awesome-Mamba-in-Low-Level-Vision" target="_blank" rel="external nofollow noopener"><b>Awesome-Mamba-in-LLV</b></a>, collecting recent Mamba-based methods! </td> </tr> <tr> <th scope="row" style="width: 150px;">Jul 01, 2024</th> <td> The first Mamba-based image restoration backbone, <strong>MambaIR</strong>, has been accepted by <strong>ECCV2024</strong>! </td> </tr> <tr> <th scope="row" style="width: 150px;">Aug 04, 2023</th> <td> One knowledge distillation based OCR for real-world degraded scene was accepted by <strong>MM2023</strong>! </td> </tr> <tr> <th scope="row" style="width: 150px;">Apr 15, 2023</th> <td> One text image super-resolution work has been accepted by <strong>IJCAI2023</strong>! </td> </tr> </table> </div> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mambairv2-480.webp 480w,/assets/img/publication_preview/mambairv2-800.webp 800w,/assets/img/publication_preview/mambairv2-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/mambairv2.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mambairv2.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="guo2024mambairv2" class="col-sm-8"> <div class="title">MambaIRv2: Attentive State Space Restoration</div> <div class="author"> <em>Hang Guo*</em> ,Â  Yong Guo* ,Â  Yaohua Zha , and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Yulun Zhang, Wenbo Li, Tao Dai, Shu-Tao Xia, Yawei Li' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/abs/2411.15269" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://zhuanlan.zhihu.com/p/9375585949" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/csguoh/MambaIR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/csguoh/MambaIR" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/csguoh/MambaIR"> </a> </div> <div class="abstract hidden"> <p>The Mamba-based image restoration backbones have recently demonstrated significant potential in balancing global reception and computational efficiency. However, the inherent causal modeling limitation of Mamba, where each token depends solely on its predecessors in the scanned sequence, restricts the full utilization of pixels across the image and thus presents new challenges in image restoration. In this work, we propose MambaIRv2, which equips Mamba with the non-causal modeling ability similar to ViTs to reach the attentive state space restoration model. Specifically, the proposed attentive state-space equation allows to attend beyond the scanned sequence and facilitate image unfolding with just one single scan. Moreover, we further introduce a semantic-guided neighboring mechanism to encourage interaction between distant but similar pixels. Extensive experiments show our MambaIRv2 outperforms SRFormer by even 0.35dB PSNR for lightweight SR even with 9.3% less parameters and suppresses HAT on classic SR by up to 0.29dB.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">guo2024mambairv2</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MambaIRv2: Attentive State Space Restoration}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guo*, Hang and Guo*, Yong and Zha, Yaohua and Zhang, Yulun and Li, Wenbo and Dai, Tao and Xia, Shu-Tao and Li, Yawei}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">repostar</span> <span class="p">=</span> <span class="s">{csguoh/MambaIR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/adaptir-480.webp 480w,/assets/img/publication_preview/adaptir-800.webp 800w,/assets/img/publication_preview/adaptir-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/adaptir.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="adaptir.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="guo2024adaptir" class="col-sm-8"> <div class="title">Parameter Efficient Adaptation for Image Restoration with Heterogeneous Mixture-of-Experts</div> <div class="author"> <em>Hang Guo</em> ,Â  Tao Dai ,Â  Yuanchao Bai , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Bin Chen, Xudong Ren, Zexuan Zhu, Shu-Tao Xia' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>The Annual Conference on Neural Information Processing Systems (NeurIPS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2312.08881.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/csguoh/AdaptIR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/csguoh/AdaptIR" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/csguoh/AdaptIR"> </a> </div> <div class="abstract hidden"> <p>Pre-training has shown promising results on various image restoration tasks, which is usually followed by full fine-tuning for each specific downstream task (e.g., image denoising). However, such full fine-tuning usually suffers from the problems of heavy computational cost in practice, due to the massive parameters of pre-trained restoration models, thus limiting its real-world applications. Recently, Parameter Efficient Transfer Learning (PETL) offers an efficient alternative solution to full fine-tuning, yet still faces great challenges for pre-trained image restoration models, due to the diversity of different degradations. To address these issues, we propose AdaptIR, a novel parameter efficient transfer learning method for adapting pre-trained restoration models. Specifically, the proposed method consists of a multi-branch inception structure to orthogonally capture local spatial, global spatial, and channel interactions. In this way, it allows powerful representations under a very low parameter budget. Extensive experiments demonstrate that the proposed method can achieve comparable or even better performance than full fine-tuning, while only using 0.6% parameters. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">guo2024adaptir</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Parameter Efficient Adaptation for Image Restoration with Heterogeneous Mixture-of-Experts}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guo, Hang and Dai, Tao and Bai, Yuanchao and Chen, Bin and Ren, Xudong and Zhu, Zexuan and Xia, Shu-Tao}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Annual Conference on Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">repostar</span> <span class="p">=</span> <span class="s">{csguoh/AdaptIR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/refir-480.webp 480w,/assets/img/publication_preview/refir-800.webp 800w,/assets/img/publication_preview/refir-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/refir.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="refir.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="guo2024refir" class="col-sm-8"> <div class="title">ReFIR: Grounding Large Restoration Models with Retrieval Augmentation</div> <div class="author"> <em>Hang Guo*</em> ,Â  Tao Dai ,Â  Zhihao Ouyang , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Taolin Zhang, Yaohua Zha, Bin Chen, Shu-Tao Xia' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>The Annual Conference on Neural Information Processing Systems (NeurIPS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2410.05601" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://zhuanlan.zhihu.com/p/1659159986" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/csguoh/ReFIR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/csguoh/ReFIR" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/csguoh/ReFIR"> </a> </div> <div class="abstract hidden"> <p>Recent advances in diffusion-based Large Restoration Models (LRMs) have significantly improved photo-realistic image restoration by leveraging the internal knowledge embedded within model weights. However, existing LRMs often suffer from the hallucination dilemma, i.e., producing incorrect contents or textures when dealing with severe degradations, due to their heavy reliance on limited internal knowledge. In this paper, we propose an orthogonal solution called the Retrieval-augmented Framework for Image Restoration (ReFIR), which incorporates retrieved images as external knowledge to extend the knowledge boundary of existing LRMs in generating details faithful to the original scene. Specifically, we first introduce the nearest neighbor lookup to retrieve content-relevant high-quality images as reference, after which we propose the cross-image injection to modify existing LRMs to utilize high-quality textures from retrieved images. Thanks to the additional external knowledge, our ReFIR can well handle the hallucination challenge and facilitate faithfully results. Extensive experiments demonstrate that ReFIR can achieve not only high-fidelity but also realistic restoration results. Importantly, our ReFIR requires no training and is adaptable to various LRMs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">guo2024refir</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ReFIR: Grounding Large Restoration Models with Retrieval Augmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guo*, Hang and Dai, Tao and Ouyang, Zhihao and Zhang, Taolin and Zha, Yaohua and Chen, Bin and Xia, Shu-Tao}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Annual Conference on Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">repostar</span> <span class="p">=</span> <span class="s">{csguoh/ReFIR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ECCV</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mambair-480.webp 480w,/assets/img/publication_preview/mambair-800.webp 800w,/assets/img/publication_preview/mambair-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/mambair.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mambair.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="guo2024mambair" class="col-sm-8"> <div class="title">MambaIR: A Simple Baseline for Image Restoration with State-Space Model</div> <div class="author"> <em>Hang Guo*</em> ,Â  Jinmin Li* ,Â  Tao Dai , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Zhihao Ouyang, Xudong Ren, Shu-Tao Xia' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Proceedings of the European Conference on Computer Vision (ECCV)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/abs/2402.15648" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://zhuanlan.zhihu.com/p/684248751" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/csguoh/MambaIR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/csguoh/MambaIR" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/csguoh/MambaIR"> </a> </div> <div class="abstract hidden"> <p>Recent years have witnessed great progress in image restoration thanks to the advancements in modern deep neural networks e.g. Convolutional Neural Network and Transformer. However, existing restoration backbones are usually limited due to the inherent local reductive bias or quadratic computational complexity. Recently, Selective Structured State Space Model e.g., Mamba, have shown great potential for long-range dependencies modeling with linear complexity, but it is still under-explored in low-level computer vision. In this work, we introduce a simple but strong benchmark model, named MambaIR, for image restoration. In detail, we propose the Residual State Space Block as the core component, which employs convolution and channel attention to enhance capabilities of the vanilla Mamba. In this way, our MambaIR takes advantages of local patch recurrence prior as well as channel interaction to produce restoration-specific feature representation. Extensive experiments demonstrate the superiority of our method, for example, MambaIR outperforms Transformer-based baseline SwinIR by up to 0.36dB, using similar computational cost but with global receptive field.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">guo2024mambair</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MambaIR: A Simple Baseline for Image Restoration with State-Space Model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guo*, Hang and Li*, Jinmin and Dai, Tao and Ouyang, Zhihao and Ren, Xudong and Xia, Shu-Tao}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">repostar</span> <span class="p">=</span> <span class="s">{csguoh/MambaIR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/latta-480.webp 480w,/assets/img/publication_preview/latta-800.webp 800w,/assets/img/publication_preview/latta-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/latta.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="latta.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="liu2024taming" class="col-sm-8"> <div class="title">Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation</div> <div class="author"> Peiyuan Liu* ,Â  <em>Hang Guo*</em> ,Â  Tao Dai , and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Naiqi Li, Jigang Bao, Xudong Ren, Yong Jiang, Shu-Tao Xia' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Association for the Advancement of Artificial Intelligence (AAAI)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/html/2403.07300v1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Hank0626/LLaTA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2024taming</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu*, Peiyuan and Guo*, Hang and Dai, Tao and Li, Naiqi and Bao, Jigang and Ren, Xudong and Jiang, Yong and Xia, Shu-Tao}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Association for the Advancement of Artificial Intelligence (AAAI)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%63%73%68%67%75%6F@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=fRwhfpoAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/csguoh" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/hang-guo-59690a2a2" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/csguoh" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://www.zhihu.com/people/hang-hang-oc" title="Blogger" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-blogger-b"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Hang Guo (éƒ­èˆª). Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>